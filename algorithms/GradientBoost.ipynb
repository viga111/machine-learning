{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56926076",
   "metadata": {},
   "source": [
    "**<font size=5 color=black>GBDT</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f7f61",
   "metadata": {},
   "source": [
    "**1** 介绍GBDT(Gradient Boosting Decision Tree)的基本原理\n",
    "\n",
    "**2** 手动实现GBDT回归算法的基本框架\n",
    "\n",
    "**3** 分别以一元和多元回归任务为例，对比本文算法与sklearn的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6bfd3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.627188Z",
     "start_time": "2021-09-21T11:36:20.128383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6ae98",
   "metadata": {},
   "source": [
    "**<font size=4 color=black>1 GBDT算法</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbdfdd",
   "metadata": {},
   "source": [
    "**<font size=3.5 color=black>1.1 思想</font>**\n",
    "\n",
    "在每次迭代过程中，通过**拟合负梯度**来学习一个弱学习器。\n",
    "\n",
    "一个简单例子：<font size=3.5 color=blue>已知小明的年龄是30岁，但GBDT模型不知道小明的年龄，那么它如何去预测小明的年龄？</font>\n",
    "\n",
    "1）首先，GBDT用第一个学习器来拟合，预测结果为20岁，发现残差为10岁；\n",
    "\n",
    "2）接着，用第二个学习器来拟合第一个学习器的残差，预测结果为6岁，发现残差为4岁；\n",
    "\n",
    "3）然后，用第三个学习器来拟合第二个学习器的残差，预测结果为3岁，发现残差为1岁。\n",
    "\n",
    "假设这时GBDT没有其他学习器了，或者误差大小已满足我们的要求，那么GBDT最后输出的预测结果为20+6+3=29岁。\n",
    "\n",
    "**<font size=3.5 color=red>说明：</font>**\n",
    "\n",
    "上述例子仅用于粗略理解GBDT的学习过程，实际上只有GBDT模型的损失函数为平方损失时，才能按上述方法进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6775d6",
   "metadata": {},
   "source": [
    "**<font size=3.5 color=black>1.2 流程</font>**\n",
    "\n",
    "GBDT可解释为以**CART回归树**为基学习器的加法模型，损失函数通常为平方损失、绝对损失等，学习算法为前向分步\n",
    "\n",
    "算法的学习方法。可用于回归和分类问题，本文仅以回归问题为例，给出GBDT算法的基本流程。\n",
    "$\\rule[1pt]{23cm}{0.1em}$\n",
    "**输入**：训练数据集$D={(x_1, y_1),(x_2, y_2),...,(x_N, y_N)},x_i \\in R^n,y_i \\in R$；损失函数为$L(y, f(x))$；<br>\n",
    "**输出**：回归树$\\hat{f}(x)$\n",
    "\n",
    "**（1）**初始化\n",
    "$$f_0(x) = \\mathop{argmin} \\limits_c \\sum_{i=1}^{N}L(y_i, c)\\tag{1}$$\n",
    "\n",
    "**（2）**假设进行$m$轮训练，$m=1,2,...,M$\n",
    "\n",
    "**（2a）**对$i=1,2,...,N$，计算负梯度\n",
    "$$r_{mi}=-\\big[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}\\big]_{f(x)=f_{m-1}(x)}\\tag{2}$$\n",
    "\n",
    "**（2b）**对${(x_1, r_{m1}),(x_2, r_{m2}),...,(x_N, r_{mN})}$拟合一个回归树，得到叶节点区域$R_{mj}$，$j=1,2,...,J$\n",
    "\n",
    "**（2c）**对$j=1,2,...,J$，计算每个叶节点的预测值，即\n",
    "$$c_{mj}=\\mathop{argmin} \\limits_{c} \\sum_{x_i \\in R_{mj}} L(y_i,f_{m-1}(x_i)+c)\\tag{3}$$\n",
    "\n",
    "**（2d）**更新\n",
    "$$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})\\tag{4}$$\n",
    "\n",
    "**（3）**得到回归树\n",
    "$$\\hat{f}(x)=f_M(x)=f_0(x)+\\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})\\tag{5}$$\n",
    "$\\rule[1pt]{23cm}{0.1em}$\n",
    "\n",
    "**<font size=3.5 color=red>几点说明：</font>**\n",
    "\n",
    "**（1）GBDT的本质在于将划分（2b）和取值（2c）分为两步进行**，即先利用回归树拟合负梯度得到叶节点区域，然后再\n",
    "\n",
    "根据式(3)计算每个叶节点的输出值。那为什么要这么做？\n",
    "\n",
    "在参数空间，对于损失函数$L(y,\\theta)$我们采用梯度下降方法按如下公式不断更新参数$\\theta$的值，来逐渐逼近$min(L(y,\\theta))$\n",
    "$$\\theta_{k+1}=\\theta_k-\\eta \\frac{\\partial L(y,\\theta_k)}{\\partial \\theta_k}\\tag{6}$$\n",
    "而在函数空间，对于损失函数$L(y,f_m(x))$，我们可以将$f_m(x)$看作一个参数，类比可以得到下面的更新公式：\n",
    "$$f_m(x)=f_{m-1}(x)-\\eta \\frac{\\partial L(y,f_{m-1}(x))}{\\partial f_{m-1}(x)}\\tag{7}$$\n",
    "令$T_m(x)=-\\eta \\frac{\\partial L(y,f_{m-1}(x))}{\\partial f_{m-1}(x)}$，则式(7)变为\n",
    "$$f_m(x)=f_{m-1}(x)+T_m(x)\\tag{8}$$\n",
    "\n",
    "这就是我们的提升树模型。\n",
    "\n",
    "在进行第m轮的训练时，到(2b)这一步根据式(2)用回归树对负梯度进行拟合，可以看到式(2)和式(8)的$T_m(x)$有一个$\\eta$的差别，只有当$\\eta$为固定值1时，二者等价。但在使用梯度下降法的时候，我们清楚对于固定步长的$\\eta$，优化效果可能不佳，因此(2c)这一步，可以理解为起到了对每个叶节点的输出值自适应步长$\\eta$的作用，使得当前模型$f_m(x)$对应的损失$L(y,f_m(x))$最小。\n",
    "\n",
    "**（2）平方损失函数**$L(y,f(x))=\\frac{1}{2}(y-f(x))^2$\n",
    "\n",
    "初始化：$f_0(x)=\\frac{1}{N}\\sum_{i=1}^{N}y_i$，即$y$的均值\n",
    "\n",
    "负梯度：$r_{mi}=y_i-f_{m-1}(x_i)$，即模型$f_{m-1}$拟合数据的残差\n",
    "\n",
    "叶节点的输出值：$c_{mj}=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f_{m-1}(x_i))$，即模型$f_{m-1}$拟合数据的残差的均值\n",
    "\n",
    "**（3）绝对损失函数**$L(y,f(x))=|y-f(x)|$\n",
    "\n",
    "初始化：$f_0(x)=median(y_i)$，即$y$的中位数\n",
    "\n",
    "负梯度：$r_{mi}=sign(y_i-f_{m-1}(x_i))$，只有1和-1两种取值\n",
    "\n",
    "叶节点的输出值：$c_{mj}=median(y_i-f_{m-1}(x_i))$，即模型$f_{m-1}$拟合数据的残差的中位数\n",
    "\n",
    "<font color=red>*注：本文主要阐述了GBDT的一些基础理论，更多关于GBDT的知识见参考。*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccd7bf",
   "metadata": {},
   "source": [
    "**<font size=4 color=black>2 手动实现GBDT回归算法</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0b17f",
   "metadata": {},
   "source": [
    "（1）本章重点在于理解GBDT算法的学习流程，对于基回归器的实现借助于[sklearn.ensemble.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "（2）为方便后续案例测试结果的对比，实现过程的一些细节适当参考了sklearn中的实现源码\n",
    "\n",
    "（3）为简化代码，本章仅对一些必要参数进行设置，仅通过具体的实现过程帮助理解算法，并不追求最终的预测精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f647f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.658824Z",
     "start_time": "2021-09-21T11:36:21.630192Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor(object):\n",
    "    \"\"\"GBDT回归器   \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss: str \n",
    "        损失函数 本文算法支持{'ls', 'lad'}，默认为'ls'\n",
    "    learning_rate: float\n",
    "        学习率 防止过拟合，默认为1.0 可以参考sklearn文档和源码了解该参数的使用\n",
    "    n_estimators: int\n",
    "        基回归器的个数\n",
    "    criterion: str\n",
    "        决策树的分裂标准 本文算法支持{'mse','friedman_mse'}，默认为'mse'\n",
    "    max_depth: int\n",
    "        决策树的最大深度\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    init_raw_predictions: ndarray of shape (n_samples,)\n",
    "        初始预测值\n",
    "    estimators_: list of DecisionTreeRegressor of shape (n_estimators,)\n",
    "        每一轮训练的基回归器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss='ls', learning_rate=1.0, n_estimators=3,\n",
    "                 criterion='mse', max_depth=1):\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.init_raw_predictions = None\n",
    "        self.estimators_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"训练回归器\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features) 样本向量\n",
    "        y: ndarray of shape (n_samples,) 目标值\n",
    "        \"\"\"\n",
    "        # 计算初始预测值\n",
    "        raw_predictions = self.get_init_raw_predictions(y)\n",
    "        self.init_raw_predictions = raw_predictions\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # 构建决策树回归器\n",
    "            estimator = DecisionTreeRegressor(criterion=self.criterion,\n",
    "                                              max_depth=self.max_depth,\n",
    "                                              random_state=42)\n",
    "            # 计算负梯度\n",
    "            neg_grad = self.negative_gradient(y, raw_predictions)\n",
    "            # 拟合负梯度\n",
    "            estimator.fit(X, neg_grad)\n",
    "            # 更新叶节点预测值\n",
    "            if self.loss == 'ls':\n",
    "                y = y - raw_predictions\n",
    "                raw_predictions = self.learning_rate * estimator.predict(X)\n",
    "            elif self.loss == 'lad':\n",
    "                y = y - raw_predictions\n",
    "                diff = y\n",
    "                raw_predictions = self.update_teminal_predictions(\n",
    "                    estimator, X, diff)\n",
    "            # 存储学习完成的回归器\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测回归值\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray of shape (n_samples, n_features) 样本向量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: ndarray of shape (n_samples,) 回归值\n",
    "        \"\"\"\n",
    "        pred = self.init_raw_predictions[:X.shape[0]]\n",
    "        for estimator in self.estimators_:\n",
    "            pred += self.learning_rate * estimator.predict(X)\n",
    "        return pred\n",
    "\n",
    "    def negative_gradient(self, y, raw_predictions):\n",
    "        \"\"\"计算负梯度\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: ndarray of shape (n_samples,) 目标值\n",
    "        raw_predictions: ndarray of shape (n_samples,) 目标值的预测值\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: ndarray of shape (n_samples,) 负梯度\n",
    "        \"\"\"\n",
    "        if self.loss == 'ls':\n",
    "            return y - raw_predictions\n",
    "        elif self.loss == 'lad':\n",
    "            return 2 * (y - raw_predictions > 0) - 1\n",
    "\n",
    "    def get_init_raw_predictions(self, y):\n",
    "        \"\"\"计算初始预测值\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: ndarray of shape (n_samples,) 目标值\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: ndarray of shape (n_samples,) 目标值的初始预测值\n",
    "        \"\"\"\n",
    "        if self.loss == 'ls':\n",
    "            return np.repeat(np.mean(y), y.shape[0])\n",
    "        elif self.loss == 'lad':\n",
    "            return np.repeat(np.median(y), y.shape[0])\n",
    "\n",
    "    def update_teminal_predictions(self, estimator, X, diff):\n",
    "        \"\"\"更新决策树叶节点的输出值。对于本文算法而言，当loss='ls'时，\n",
    "        不需要更新叶节点的输出值，仅loss='lad'时需要更新叶节点的输出值。\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator: class 决策树回归器\n",
    "        X: ndarray of shape (n_samples, n_features) 样本向量\n",
    "        diff: ndarray of shape (n_samples,) diff = y - raw_predictions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: ndarray of shape (n_samples,) 预测值\n",
    "        \"\"\"\n",
    "        leaves_index = estimator.apply(X)  # 获取每个样本落入的叶节点id\n",
    "        for i in np.unique(leaves_index):\n",
    "            ind = np.where(leaves_index == i)[0]\n",
    "            # 计算中位数 参考sklearn中个数为偶数时中位数取较小的那个\n",
    "            sort_diff = np.sort(diff[ind])\n",
    "            med_ind = (sort_diff.shape[0] - 1) // 2 \n",
    "            # 更新叶节点的输出值\n",
    "            estimator.tree_.value[i] = sort_diff[med_ind]\n",
    "        return self.learning_rate * estimator.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0e7da",
   "metadata": {},
   "source": [
    "**<font size=4 colot=black>3 案例</font>**\n",
    "\n",
    "**<font size=3.5 colot=black>3.1 一元回归</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72ea5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.674825Z",
     "start_time": "2021-09-21T11:36:21.661825Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([5.56, 5.70, 5.91, 6.40, 6.80, 7.05, 8.90, 8.70, 9.00, 9.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448a7b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.706825Z",
     "start_time": "2021-09-21T11:36:21.677826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本文算法\n",
      "-------\n",
      "集成回归器预测结果：\n",
      " [5.56 5.56 5.91 6.4  6.4  8.7  8.7  8.7  9.   9.05]\n",
      "基回归器1预测结果：\n",
      " [-1.015 -1.015 -1.015 -1.015 -1.015  1.975  1.975  1.975  1.975  1.975]\n",
      "基回归器2预测结果：\n",
      " [-0.21 -0.21 -0.21  0.49  0.49 -0.2  -0.2  -0.2   0.1   0.1 ]\n",
      "基回归器3预测结果：\n",
      " [-0.14 -0.14  0.21  0.    0.    0.    0.    0.    0.    0.05]\n"
     ]
    }
   ],
   "source": [
    "# 本文算法\n",
    "myreg = MyGradientBoostingRegressor(loss='lad', max_depth=3)\n",
    "myreg.fit(X, y)\n",
    "\n",
    "print('本文算法')\n",
    "print('-------')\n",
    "print('集成回归器预测结果：\\n', myreg.predict(X))\n",
    "print('基回归器1预测结果：\\n', myreg.estimators_[0].predict(X))\n",
    "print('基回归器2预测结果：\\n', myreg.estimators_[1].predict(X))\n",
    "print('基回归器3预测结果：\\n', myreg.estimators_[2].predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b32d49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.738833Z",
     "start_time": "2021-09-21T11:36:21.709830Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n",
      "-------\n",
      "集成回归器预测结果：\n",
      " [5.56 5.56 5.91 6.4  6.4  8.7  8.7  8.7  9.   9.05]\n",
      "基回归器1预测结果：\n",
      " [-1.015 -1.015 -1.015 -1.015 -1.015  1.975  1.975  1.975  1.975  1.975]\n",
      "基回归器2预测结果：\n",
      " [-0.21 -0.21 -0.21  0.49  0.49 -0.2  -0.2  -0.2   0.1   0.1 ]\n",
      "基回归器3预测结果：\n",
      " [-0.14 -0.14  0.21  0.    0.    0.    0.    0.    0.    0.05]\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "reg = GradientBoostingRegressor(loss='lad', random_state=42, n_estimators=3,\n",
    "                                learning_rate=1.0, max_depth=3, criterion='mse')\n",
    "reg.fit(X, y)\n",
    "\n",
    "print('sklearn')\n",
    "print('-------')\n",
    "print('集成回归器预测结果：\\n', reg.predict(X))\n",
    "print('基回归器1预测结果：\\n', reg.estimators_[0][0].predict(X))\n",
    "print('基回归器2预测结果：\\n', reg.estimators_[1][0].predict(X))\n",
    "print('基回归器3预测结果：\\n', reg.estimators_[2][0].predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695552fc",
   "metadata": {},
   "source": [
    "可以看出，对于一元回归问题，本文算法和sklearn的结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964e8e1",
   "metadata": {},
   "source": [
    "**<font size=3.5 colot=black>3.2 多元回归</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed75c80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.754833Z",
     "start_time": "2021-09-21T11:36:21.741836Z"
    }
   },
   "outputs": [],
   "source": [
    "# 波士顿房价数据集\n",
    "boston = load_boston()\n",
    "X2 = boston.data\n",
    "y2 = boston.target\n",
    "X2_test = X2[[0, 100, 200], :]  # 为方便显示，选取三个样本用于预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b690fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.786831Z",
     "start_time": "2021-09-21T11:36:21.758834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本文算法\n",
      "-------\n",
      "集成回归器预测结果：\n",
      " [28.65131748 20.89789538 33.39272232]\n",
      "基回归器1预测结果：\n",
      " [ 0.37239368  0.37239368 10.81603089]\n",
      "基回归器2预测结果：\n",
      " [ 5.45614711 -0.36700372 -0.24608527]\n",
      "基回归器3预测结果：\n",
      " [ 0.28997038 -1.6403009   0.28997038]\n"
     ]
    }
   ],
   "source": [
    "# 本文算法\n",
    "myreg2 = MyGradientBoostingRegressor(loss='ls', max_depth=3)\n",
    "myreg2.fit(X2, y2)\n",
    "\n",
    "print('本文算法')\n",
    "print('-------')\n",
    "print('集成回归器预测结果：\\n', myreg2.predict(X2_test))\n",
    "print('基回归器1预测结果：\\n', myreg2.estimators_[0].predict(X2_test))\n",
    "print('基回归器2预测结果：\\n', myreg2.estimators_[1].predict(X2_test))\n",
    "print('基回归器3预测结果：\\n', myreg2.estimators_[2].predict(X2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cf8a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:36:21.818834Z",
     "start_time": "2021-09-21T11:36:21.789837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n",
      "-------\n",
      "集成回归器预测结果：\n",
      " [28.65131748 20.89789538 33.39272232]\n",
      "基回归器1预测结果：\n",
      " [ 0.37239368  0.37239368 10.81603089]\n",
      "基回归器2预测结果：\n",
      " [ 5.45614711 -0.36700372 -0.24608527]\n",
      "基回归器3预测结果：\n",
      " [ 0.28997038 -1.6403009   0.28997038]\n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "reg2 = GradientBoostingRegressor(loss='ls', random_state=42, n_estimators=3,\n",
    "                                 learning_rate=1.0, max_depth=3, criterion='mse')\n",
    "reg2.fit(X2, y2)\n",
    "\n",
    "print('sklearn')\n",
    "print('-------')\n",
    "print('集成回归器预测结果：\\n', reg2.predict(X2_test))\n",
    "print('基回归器1预测结果：\\n', reg2.estimators_[0][0].predict(X2_test))\n",
    "print('基回归器2预测结果：\\n', reg2.estimators_[1][0].predict(X2_test))\n",
    "print('基回归器3预测结果：\\n', reg2.estimators_[2][0].predict(X2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43492db7",
   "metadata": {},
   "source": [
    "可以看出，对于多元回归问题，本文算法和sklearn的结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad9dec",
   "metadata": {},
   "source": [
    "**<font color=black size=4>参考</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1973e6e",
   "metadata": {},
   "source": [
    "1 李航. (2012) 统计学习方法. 清华大学出版社, 北京.\n",
    "\n",
    "2 [Friedman J H . Greedy Function Approximation: A Gradient Boosting Machine[J]. Annals of Statistics, 2001, 29(5):1189-1232.](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n",
    "\n",
    "3 [sklearn.ensemble.GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "\n",
    "4 [sklearn.ensemble.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "5 [sklearn-criterion](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_criterion.pyx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "308.667px",
    "left": "897px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
